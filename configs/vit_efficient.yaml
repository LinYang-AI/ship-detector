# Efficient ViT Configuration with LoRA and Memory Optimization

model:
  name: vit_base_patch16_224  # Model architecture
  pretrained: true  # Use pretrained weights
  
  # LoRA Configuration
  use_lora: true  # Enable LoRA for parameter-efficient training
  lora_rank: 16  # Low-rank dimension (lower = fewer parameters)
  lora_alpha: 16.0  # Scaling factor
  lora_target_modules:  # Which modules to apply LoRA to
    - qkv  # Query, Key, Value projections
    - proj  # Output projection
    - fc1  # FFN layer 1
    - fc2  # FFN layer 2
  
  # Memory optimization
  gradient_checkpointing: false  # Enable for very large models
  freeze_backbone_epochs: 0  # Keep at 0 when using LoRA

training:
  batch_size: 32  # Adjust based on GPU memory
  max_epochs: 30
  early_stopping_patience: 5
  pos_weight: 3.0  # Weight for positive class
  
  # Memory optimization
  accumulate_grad_batches: 4  # Gradient accumulation (effective batch = 32 * 4 = 128)
  gradient_clip_val: 1.0  # Gradient clipping
  precision: 16  # Mixed precision training (16 or 32)
  
  # Training limits (for debugging or quick tests)
  limit_train_batches: 1.0  # Use full training set (set < 1.0 to use subset)
  limit_val_batches: 1.0  # Use full validation set
  val_check_interval: 1.0  # Validate every epoch (can be float for partial)

optimizer:
  name: adamw  # Options: adam, adamw, 8bit_adam (requires bitsandbytes)
  lr: 5e-4  # Higher LR works well with LoRA
  weight_decay: 0.01

scheduler:
  name: cosine
  T_max: 30  # Should match max_epochs
  eta_min: 1e-6

data:
  # Memory management
  use_streaming: false  # Enable for very large datasets
  cache_size: 100  # Number of images to cache (0 = no caching)
  chunk_size: 1000  # For streaming dataset
  
  # DataLoader settings
  num_workers: 2  # Reduce if CPU usage is high
  pin_memory: true  # Faster GPU transfer
  persistent_workers: false  # Keep workers alive between epochs
  prefetch_factor: 2  # Number of batches to prefetch per worker
  
  # Data split
  val_split: 0.2
  random_seed: 42

# Memory usage profiles
profiles:
  low_memory:  # For systems with < 16GB RAM
    training:
      batch_size: 16
      accumulate_grad_batches: 8
    data:
      cache_size: 0
      num_workers: 1
      persistent_workers: false
      use_streaming: true
    model:
      use_lora: true
      lora_rank: 8
  
  balanced:  # For systems with 16-32GB RAM
    training:
      batch_size: 32
      accumulate_grad_batches: 4
    data:
      cache_size: 100
      num_workers: 2
      persistent_workers: false
      use_streaming: false
    model:
      use_lora: true
      lora_rank: 16
  
  high_performance:  # For systems with > 32GB RAM
    training:
      batch_size: 64
      accumulate_grad_batches: 2
    data:
      cache_size: 500
      num_workers: 4
      persistent_workers: true
      use_streaming: false
    model:
      use_lora: false  # Can afford full fine-tuning
      lora_rank: 32