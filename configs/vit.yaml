# ViT Ship Classifier Configuration

model:
  name: vit_base_patch16_224.mae  # Options: vit_small_patch16_224, vit_base_patch16_224, vit_large_patch16_224
  pretrained: true  # Use ImageNet pretrained weights
  freeze_backbone_epochs: 2  # Freeze backbone for first N epochs (0 to disable)

training:
  batch_size: 32  # Reduce if GPU OOM
  max_epochs: 30
  early_stopping_patience: 5
  pos_weight: 3.0  # Weight for positive class (ships) to handle imbalance
  use_weighted_sampler: true  # Use weighted sampling for class balance
  precision: 16  # Mixed precision training (16 or 32)

optimizer:
  name: adamw  # Options: adam, adamw
  lr: 1e-4  # Initial learning rate
  weight_decay: 0.01  # L2 regularization

scheduler:
  name: cosine  # Options: cosine, reduce_on_plateau
  T_max: 30  # For cosine annealing (should match max_epochs)
  eta_min: 1e-6  # Minimum learning rate
  # For reduce_on_plateau:
  # factor: 0.5
  # patience: 5
  # min_lr: 1e-7

augmentation:
  hflip_prob: 0.5  # Horizontal flip probability
  vflip_prob: 0.5  # Vertical flip probability
  rotation: true  # Random 90-degree rotations
  color_jitter: true  # Color augmentation

data:
  val_split: 0.2  # Validation set proportion
  num_workers: 4  # DataLoader workers
  random_seed: 42  # For reproducibility

# Alternative lightweight config for testing
# model:
#   name: vit_tiny_patch16_224  # Smaller model
#   pretrained: true
#   freeze_backbone_epochs: 0
# 
# training:
#   batch_size: 64
#   max_epochs: 5
#   early_stopping_patience: 3
#   pos_weight: 1.0
#   use_weighted_sampler: false
#   precision: 32